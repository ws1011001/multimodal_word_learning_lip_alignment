{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c0acc7",
   "metadata": {},
   "source": [
    "## Crop and align talking videos according to lip-movement\n",
    "\n",
    "This is a part of the multimodal word learning project. The original talking videos contain the whole face of the speaker. To prepare video stimuli for fMRI experiment, we need to crop the original videos so that only the lip area remained, and to align videos based on the mouth position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d1d563",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load up packages\n",
    "import os\n",
    "import dlib  # use the pre-trained predictor in dlib for detecting facial landmarks, see # http://dlib.net/face_landmark_detection.py.html\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from moviepy.editor import *\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac71dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup working path\n",
    "dir_project = '/Projects/multimodal_word_learning'\n",
    "dir_stimuli = os.path.join(dir_project, 'experiment_design/stimuli')\n",
    "f_predictor = os.path.join(dir_stimuli, 'shape_predictor_68_face_landmarks.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f1ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utils\n",
    "def detect_movements(img_dir, img_name, predictor_path, n_img):\n",
    "    '''\n",
    "    Function to detect horizontal and vertical lip movements by using dlib's 68-face-landmarks predictor.\n",
    "    '''\n",
    "    # Load up predictor\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(predictor_path)\n",
    "    # Detect facial landmarks for each frame\n",
    "    mv_v = np.zeros(n_img)  # vertical lip movements\n",
    "    mv_h = np.zeros(n_img)  # horizontal lip movements\n",
    "    for i in range(n_img):\n",
    "        f_img = os.path.join(img_dir, img_name, f'{img_name}_out-{i+1:04d}.png')\n",
    "        img = dlib.load_rgb_image(f_img)\n",
    "        box = detector(img, 1)\n",
    "        shape = predictor(img, box[0])\n",
    "        # Calculate vertical and horizontal lip movements\n",
    "        mv_v[i] = np.abs(shape.part(57).y - shape.part(51).y)\n",
    "        mv_h[i] = np.abs(shape.part(64).x - shape.part(60).x)\n",
    "        \n",
    "    return mv_v, mv_h\n",
    "\n",
    "\n",
    "def detect_shift(shape, ref_shape=None):\n",
    "    shape = shape_vector(shape)\n",
    "    center = shape[48:, :].mean(axis=0)  # center of mass\n",
    "    ref_shape = shape_vector(ref_shape)\n",
    "    print(f'The shape center is {center}.')\n",
    "    if ref_shape is not None:\n",
    "        ref_center = ref_shape[48:, :].mean(axis=0)\n",
    "        x_shift = np.round(ref_center[0] - center[0])  # at least 1 pixel\n",
    "        y_shift = np.round(ref_center[1] - center[1])\n",
    "        print(f'The reference is {ref_center}. X shift {x_shift}, Y shift {y_shift}.')\n",
    "    else:\n",
    "        x_shift, y_shift = 0, 0\n",
    "        \n",
    "    return center, x_shift, y_shift\n",
    "        \n",
    "\n",
    "def shape_vector(shape):\n",
    "    V = np.zeros((68, 2))\n",
    "    for i in range(68):\n",
    "        V[i, 0] = shape.part(i).x  # x coordinate\n",
    "        V[i, 1] = shape.part(i).y  # y coordinate\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf547c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read up video information\n",
    "df_vids = pd.read_csv(os.path.join(dir_stimuli, 'stimuli_videos.csv'))\n",
    "dur_video = 1760  # video duration in ms\n",
    "dur_frame = 5     # frame resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f35745",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clip videos according to the onset and offset of lip-movements (i.e., articulation)\n",
    "df_vids['movement_vertical'] = [[] for _ in range(len(df_vids))]\n",
    "df_vids['movement_horizontal'] = [[] for _ in range(len(df_vids))]\n",
    "for i in tqdm(range(len(df_vids))):\n",
    "    i_stim = df_vids.loc[i, 'stimulus']  # video name\n",
    "    print(f\"Clip the video {i_stim}.\")\n",
    "    # Detect movements\n",
    "    v, h = detect_movements(os.path.join(dir_stimuli, 'videos'), i_stim, f_predictor)  \n",
    "    df_vids.at[i, 'movement_vertical'] = list(v)\n",
    "    df_vids.at[i, 'movement_horizontal'] = list(h)\n",
    "    # Plot movements\n",
    "    T = np.linspace(0, dur_video, np.ceil(dur_video / dur_frame))\n",
    "    aud_onset = np.floor(df_vids.loc[i, 'audio_onset'] / dur_frame)\n",
    "    aud_offset = np.ceil(df_vids.loc[i, 'audio_offset'] / dur_frame)\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(T, v, 'r', label='Vertical')    # vertical movements\n",
    "    ax.plot(T, h, 'b', label='Horizontal')  # horizontal movements\n",
    "    ax.plot(aud_onset * dur_frame, v[int(aud_onset)], 'Xg', ms=8)    # audio onset mark\n",
    "    ax.plot(aud_offset * dur_frame, v[int(aud_offset)], 'Xg', ms=8)  # audio offset mark\n",
    "    ax.set_xlabel('Time (ms)')\n",
    "    ax.set_ylabel('Pixels')\n",
    "    ax.legend()\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.savefig(os.path.join(dir_stimuli, 'videos', f'{i_stim}_movements.png'), dpi=300, transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604852a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output video information: have to manually identify the onset and offset of lip-movements/articulation\n",
    "df_vids.to_csv(os.path.join(dir_stimuli, 'stimuli_videos.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e8346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut out videos\n",
    "df_vids = pd.read_csv(os.path.join(dir_stimuli, 'stimuli_videos.csv'))  # with the manully identified onsets and offsets\n",
    "if 'video_init_dur' not in df_vids.columns:\n",
    "    df_vids['video_init_dur'] = np.zeros(len(df_vids))\n",
    "    df_vids['video_cut_dur'] = np.zeros(len(df_vids))\n",
    "for i in tqdm(range(len(df_vids))):\n",
    "    i_stim = df_vids.loc[i, 'stimulus']\n",
    "    f_video = os.path.join(dir_stimuli, 'videos', f'{i_stim}_slient.avi')\n",
    "    vid = VideoFileClip(f_video)\n",
    "    vid_onset = (df_vids.loc[i, 'video_onset']-1) * dur_frame/1000\n",
    "    vid_offset = (df_vids.loc[i, 'video_offset']-1) * dur_frame/1000 + 0.15  # extend the current offset by 150ms\n",
    "    if vid_offset > dur_video: vid_offset = dur_video                        # maximal duration\n",
    "    vid_cut = vid.subclip(vid_onset, vid_offset)\n",
    "    vid_cut.write_videofile(os.path.join(dir_stimuli, 'videos', f'{i_stim}_silent_cut.avi'), codec='libx264')\n",
    "    df_vids.loc[i, 'video_init_dur'] = vid.duration\n",
    "    df_vids.loc[i, 'video_cut_dur'] = vid_cut.duration\n",
    "    \n",
    "# Output video information\n",
    "df_vids.to_csv(os.path.join(dir_stimuli, 'stimuli_videos.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a64d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Align videos\n",
    "# Load up predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(f_predictor)\n",
    "# Crop and align videos according to the mouth position \n",
    "if 'video_x_center' not in df_vids.columns:\n",
    "    df_vids['video_x_center'] = np.zeros(len(df_vids))\n",
    "    df_vids['video_y_center'] = np.zeros(len(df_vids))\n",
    "for i in tqdm(range(len(df_vids))):\n",
    "    # Extract facial landmarks for this video\n",
    "    i_stim = df_vids.loc[i, 'stimulus']\n",
    "    print(f\"Crop the video {i_stim}.\")\n",
    "    idx = df_vids.loc[i, 'video_onset']\n",
    "    frm = os.path.join(dir_stimuli, 'videos', i_stim, f'{i_stim}_out-{idx:04d}.png')  # reference frame\n",
    "    img = dlib.load_rgb_image(frm)\n",
    "    box = detector(img, 1)\n",
    "    shape = predictor(img, box[0])\n",
    "    # Calculate the center of mass (lip)\n",
    "    center = detect_shift(shape)\n",
    "    df_vids.loc[i, 'video_x_center'] = center[0]\n",
    "    df_vids.loc[i, 'video_y_center'] = center[1]\n",
    "    # Crop this video\n",
    "    f_video = os.path.join(dir_stimuli, 'videos', f'{i_stim}_silent_cut.avi')\n",
    "    vid = VideoFileClip(f_video)\n",
    "    vid_lip = vid.crop(x1=center[0] - 250, y1=center[1] - 100, \n",
    "                       x2=center[0] + 250, y2=center[1] + 110)  # align to the center of the lip\n",
    "    vid_lip.write_videofile(os.path.join(dir_stimuli, 'videos', f'{i_stim}_lip.avi'), codec='libx264')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5ba039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output video information\n",
    "df_vids.to_csv(os.path.join(dir_stimuli, 'stimuli_videos.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
